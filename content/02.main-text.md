## Introduction {.page_break_before}

Gene expression datasets are typically "wide", with more gene features than samples.
These feature-rich and sample-poor datasets present challenges in many aspects of machine learning, including overfitting, multicollinearity, and interpretation.
To facilitate the use of feature-rich gene expression data in machine learning models, feature selection and/or dimension reduction are commonly used to distill a more condensed data representation from the input space of all genes.
The idea is that many gene expression features are likely to be redundant or irrelevant to the prediction problem of interest, so selecting or transforming them can generate a more relevant set of genes or variables.

In cancer transcriptomics, this preference for small, parsimonious sets of genes can be seen in the popularity of "gene signatures".
These are groups of genes whose expression levels are used to define cancer subtypes or to predict prognosis or therapeutic response [@doi:10.1038/nrg.2017.96; @doi:10.1016/j.ejca.2013.02.021].
Many studies specify the size of the signature in the paper's title or abstract, suggesting that the fewer genes in a gene signature, the better [@doi:10.1056/NEJMoa060096; @doi:10.1158/0008-5472.CAN-08-0436; @doi:10.1056/NEJMoa1602253].
Clinically, there are many reasons why a smaller gene signature could be better, including cost (the expression of fewer genes can be profiled using PCR or immunohistochemistry, whereas a large signature likely requires a targeted array or NGS analysis [@doi:10.1586/erm.09.32]) and interpretability (it is easier to reason about the function and biological role of a smaller gene set than a large one).
However, there is also an underlying assumption that smaller gene signatures tend to be more robust: that for a new patient or in a new biological context, a smaller gene set is more likely to maintain its predictive performance than a larger one.

In this study, we tested this robustness assumption directly by using two large, heterogeneous public cancer datasets to study model generalization across biological contexts: The Cancer Genome Atlas (TCGA) for human tumor sample data [@doi:10.1038/ng.2764], and the Cancer Cell Line Encyclopedia (CCLE) for human cell line data [@doi:10.1038/s41586-019-1186-3].
These datasets contain overlapping -omics data types derived from distinct data sources, allowing us to quantify model generalization across data sources.
In addition, each dataset contains samples from a wide range of different cancer types/tissues of origin, allowing us to quantify model generalization across cancer types.
We trained both linear and non-linear models to predict mutation status (presence or absence) from RNA-seq gene expression for 71 cancer driver genes, across varying levels of model simplicity and degrees of regularization, resulting in a variety of gene signature sizes.
We compared two simple procedures for model selection, one that combines cross-validation performance with model parsimony and one that only relies on cross-validation performance, for each classifier in each context.

Our results suggest that, in general, mutation status classification models that perform well in cross-validationwithin a biological context also generalize well across biological contexts.
There are some genes, and some cancer types, where more regularized well-performing models outperform the best-performing model, but we do not see systematic evidence of a generalization advantage for smaller/more regularized models across all genes and cancer types.
Based on our results, for building machine learning models on cancer transcriptomic data, good cross-validation performance within a biological context (data source or cancer type) is a sufficient proxy for robust performance across contexts.


## Results {.page_break_before}

### Measuring generalization using open cancer genomics data

Figure 1: graphical explanation of experimental design

### Generalization across datasets

Figure 2: summary of TCGA -> CCLE and CCLE -> TCGA results, by gene

Figure 3: explanation of "best" vs. "smallest good", distribution plot, examples

### Generalization across cancer types

Figure 4: cancer type generalization distribution plot, examples, cancer type representation analysis (which cancer types are hardest to generalize to?)

### Extension to nonlinear models

Figure 5: neural network results for varying dropout/other regularization

supplement: MSI prediction results, LASSO penalty vs. feature count plots, individual per-gene performance plots, feature count binning results


## Discussion {.page_break_before}


## Methods {.page_break_before}

### Data download and preprocessing

### Cancer gene set construction

Vogelstein gene set

### Classifier setup and cross-validation design

LASSO logistic regression details
LASSO range selection

### "Best model" vs. "smallest good model" analysis details

Description of "smallest good" heuristic
Statistical testing?

### Open science/reproducibility stuff
