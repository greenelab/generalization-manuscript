## The bigger picture {.page_break_before}

Existing recommendations in statistics and machine learning suggest that smaller, or simpler, predictive models are more likely to generalize well.
In cancer transcriptomics, this manifests as a preference for small "gene signatures", or groups of genes whose expression is used to define subtypes or suggest therapeutic interventions.
This study uses public datasets to test the generalization performance of cancer gene expression-based predictive models, both across datasets (from cell lines to tumor samples, and vice-versa), and across cancer types/tissues of origin.
In general, we do not observe strong evidence that simpler models inherently generalize more effectively than more complex ones.
Our results underscore the importance of defining clear goals in machine learning-based transcriptomic analyses.
If the goal is to achieve robust performance across contexts or datasets, whenever possible we recommend directly evaluating generalization; otherwise, we recommend choosing the model that performs the best on unseen data via cross-validation.


## Highlights

* Systematic evaluation of generalization of cancer transcriptomics predictive models  
* Cross-validation performance is equally indicative as model size/complexity  
* Similar results hold broadly across generalization contexts and model types

## Summary

Guidelines in statistical modeling for genomics hold that simpler models have advantages over more complex ones.
Potential advantages include cost, interpretability, and improved generalization across datasets or biological contexts.
We directly tested the assumption that small gene signatures generalize better by examining the generalization of mutation status prediction models across datasets (from cell lines to human tumors and vice-versa) and biological contexts (holding out entire cancer types from pan-cancer data).
We compared model selection between solely cross-validation performance and combining cross-validation performance with regularization strength.
We did not observe that more regularized signatures generalized better.
This result held across both generalization problems and for both linear models (LASSO logistic regression) and non-linear ones (neural networks).
When the goal of an analysis is to produce generalizable predictive models, we recommend choosing the ones that perform best on held-out data or in cross-validation instead of those that are smaller or more regularized.

