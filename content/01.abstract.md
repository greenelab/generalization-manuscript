## The bigger picture {.page_break_before}

Existing recommendations in statistics and machine learning suggest that smaller, or simpler, predictive models are more likely to generalize well.
In cancer transcriptomics, this manifests as a preference for small "gene signatures", or groups of genes whose expression is used to define subtypes or suggest therapeutic interventions.
This study uses public datasets to test the generalization performance of cancer gene expression-based predictive models, both across datasets (from cell lines to tumor samples, and vice-versa), and across cancer types/tissues of origin.
In general, we do not observe strong evidence that simpler models inherently generalize more effectively than more complex ones.
Our results underscore the importance of defining clear goals in machine learning-based transcriptomic analyses.
If the goal is to achieve robust performance across contexts or datasets, whenever possible we recommend directly evaluating generalization; otherwise, we recommend choosing the model that performs the best on unseen data via cross-validation.


<!-- Without directly evaluating model generalization, it is tempting to assume that simpler models will generalize better than more complex models.
Studies in the statistics and machine learning literature suggest this rule of thumb [@doi:10.1214/088342306000000060; @doi:10/bhfhgd; @doi:10.4137/CIN.S408; @doi:10.1371/journal.pcbi.1004961], and model selection approaches sometimes incorporate criteria to encourage simpler models that do not fit the data as closely.
These ideas have taken root in genomics, although they are less commonly stated formally or studied systematically [@doi:10.1007/s00405-021-06717-5; @doi:10.1089/dna.2020.6193; @doi:10.1186/s12859-021-04503-y].
However, we do not observe strong evidence that simpler models inherently generalize more effectively than more complex ones.
There may be other reasons to train small models or to look for the best model of a certain size/sparsity, such as biomarker interpretability or assay cost.
Our results underscore the importance of defining clear goals for each analysis.
If the goal is to achieve generalization across contexts or datasets, whenever possible we recommend directly evaluating generalization.
When it is not feasible, we recommend choosing the model that performs the best on unseen data via cross-validation or a holdout dataset. -->


## Highlights

* Systematic evaluation of generalization of cancer transcriptomics predictive models  
* Cross-validation performance is equally indicative as model size/complexity  
* Similar results hold broadly across generalization contexts and model types

## Summary

Guidelines in statistical modeling for genomics hold that simpler models have advantages over more complex ones.
Potential advantages include cost, interpretability, and improved generalization across datasets or biological contexts.
Gene signatures in cancer transcriptomics tend to include small subsets of genes for these reasons, and algorithms for defining signatures are often designed with these ideas in mind.
To directly test the latter assumption, that small gene signatures generalize better, we examined the generalization of mutation status prediction models across datasets (from cell lines to human tumors and vice-versa) and biological contexts (holding out entire cancer types from pan-cancer data).
We compared two simple procedures for model selection, one that exclusively relies on cross-validation performance and one that combines cross-validation performance with regularization strength.
We did not observe that more regularized signatures generalized better.
This result held across both generalization problems and for both linear models (LASSO logistic regression) and non-linear ones (neural networks).
When the goal of an analysis is to produce generalizable predictive models, we recommend choosing the ones that perform best on held-out data or in cross-validation, instead of those that are smaller or more regularized.

